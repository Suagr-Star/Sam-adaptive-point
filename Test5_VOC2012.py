import os
import random
import csv
import time
import numpy as np
import cv2
import torch
import xml.etree.ElementTree as ET
from segment_anything import sam_model_registry, SamPredictor
from tqdm import tqdm  # æ–°å¢è¿›åº¦æ¡ï¼Œæå‡ä½“éªŒ

# ====================== 1. æç®€é…ç½®ï¼ˆæ˜“ä¿®æ”¹+é˜²å‡ºé”™ï¼‰ ======================
class Config:
    # æ•°æ®é›†è·¯å¾„ï¼ˆæ›¿æ¢ä¸ºä½ çš„è·¯å¾„ï¼‰
    VOC2012_ROOT = r"E:\\SAM_Model\\datasets\\VOC2012"
    # SAMæ¨¡å‹é…ç½®
    SAM_CKPT = r"E:\\SAM_Model\\weights\\sam_vit_b_01ec64.pth"
    MODEL_TYPE = "vit_b"
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    # å®éªŒå‚æ•°ï¼ˆæ ¸å¿ƒï¼šå¼ºåˆ¶å¯ç”¨ä¼ªæ©ç ï¼Œç¡®ä¿èƒ½è¿è¡Œï¼‰
    SAMPLE_NUM = 500    # é‡‡æ ·å›¾ç‰‡æ•°
    SEED = 42           # éšæœºç§å­
    MIN_INST_AREA = 10  # æœ€å°å®ä¾‹é¢ç§¯ï¼ˆæ”¾å®½ï¼‰
    USE_FAKE_MASK = True  # æ— çœŸå®æ©ç æ—¶ï¼Œç”¨bboxç”Ÿæˆä¼ªæ©ç ï¼ˆå¼ºåˆ¶å¼€å¯ï¼‰
    # ç»“æœä¿å­˜
    OUTPUT_DIR = r"E:\\SAM_Model\\results\\voc2012_exp_fixed"

# ====================== 2. å·¥å…·å‡½æ•°ï¼ˆç®€åŒ–+é²æ£’ï¼‰ ======================
def init_env():
    """åˆå§‹åŒ–ç¯å¢ƒï¼šåˆ›å»ºç›®å½•ã€è®¾ç½®éšæœºç§å­"""
    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)
    random.seed(Config.SEED)
    np.random.seed(Config.SEED)
    torch.manual_seed(Config.SEED)
    if Config.DEVICE == "cuda":
        torch.cuda.manual_seed(Config.SEED)
    # åˆå§‹åŒ–ç»“æœæ–‡ä»¶
    with open(os.path.join(Config.OUTPUT_DIR, "image_details.csv"), "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["img_id", "valid_inst", "hard_inst", "hard_ratio", "is_complex", 
                         "point_only_miou", "adaptive_miou", "improvement"])
    with open(os.path.join(Config.OUTPUT_DIR, "log.txt"), "w", encoding="utf-8") as f:
        f.write(f"å®éªŒå¼€å§‹ï¼š{time.strftime('%Y-%m-%d %H:%M:%S')}\n")
    print("âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")

def log(msg):
    """ç®€æ˜“æ—¥å¿—ï¼šæ‰“å°+å†™å…¥æ–‡ä»¶"""
    print(msg)
    with open(os.path.join(Config.OUTPUT_DIR, "log.txt"), "a", encoding="utf-8") as f:
        f.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {msg}\n")

def save_summary(summary):
    """ä¿å­˜æ±‡æ€»ç»“æœ"""
    with open(os.path.join(Config.OUTPUT_DIR, "summary.csv"), "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f)
        for k, v in summary.items():
            writer.writerow([k, v])
    log(f"ğŸ“Š æ±‡æ€»ç»“æœå·²ä¿å­˜ï¼š{os.path.join(Config.OUTPUT_DIR, 'summary.csv')}")

# ====================== 3. VOCæ•°æ®å¤„ç†ï¼ˆæ ¸å¿ƒï¼šå…¼å®¹ä¼ªæ©ç ï¼‰ ======================
def parse_voc_xml(xml_path):
    """è§£æVOC XMLï¼Œè¿”å›å›¾ç‰‡ä¿¡æ¯+å®ä¾‹åˆ—è¡¨"""
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()
        img_info = {
            "filename": root.find("filename").text,
            "w": int(root.find("size/width").text),
            "h": int(root.find("size/height").text)
        }
        instances = []
        for obj in root.findall("object"):
            bndbox = obj.find("bndbox")
            bbox = [
                int(bndbox.find("xmin").text),
                int(bndbox.find("ymin").text),
                int(bndbox.find("xmax").text),
                int(bndbox.find("ymax").text)
            ]
            # è¿‡æ»¤æ— æ•ˆbbox
            if bbox[2] <= bbox[0] or bbox[3] <= bbox[1]:
                continue
            instances.append({
                "bbox": bbox,
                "difficult": int(obj.find("difficult").text) if obj.find("difficult") is not None else 0,
                "name": obj.find("name").text
            })
        return img_info, instances
    except Exception as e:
        log(f"âŒ è§£æXMLå¤±è´¥ {xml_path}ï¼š{str(e)}")
        return None, []

def get_instance_mask(img_name, bbox, img_h, img_w):
    """
    è·å–å®ä¾‹æ©ç ï¼ˆä¼˜å…ˆçœŸå®æ©ç ï¼Œæ— åˆ™ç”Ÿæˆä¼ªæ©ç ï¼‰
    :return: æ©ç çŸ©é˜µ (h, w) uint8ï¼Œ0=èƒŒæ™¯ï¼Œ1=å®ä¾‹
    """
    # 1. å°è¯•åŠ è½½çœŸå®æ©ç 
    seg_path = os.path.join(Config.VOC2012_ROOT, "SegmentationObject", img_name.replace(".jpg", ".png"))
    if os.path.exists(seg_path):
        seg_mask = cv2.imread(seg_path, cv2.IMREAD_GRAYSCALE)
        if seg_mask is not None:
            # æå–bboxå†…çš„å®ä¾‹ç°åº¦å€¼
            x1, y1, x2, y2 = bbox
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(img_w-1, x2), min(img_h-1, y2)
            bbox_mask = seg_mask[y1:y2, x1:x2]
            unique_gray = np.unique(bbox_mask[bbox_mask != 0])
            if len(unique_gray) > 0:
                target_gray = max(unique_gray, key=lambda g: np.sum(bbox_mask == g))
                inst_mask = (seg_mask == target_gray).astype(np.uint8)
                if np.sum(inst_mask) >= Config.MIN_INST_AREA:
                    return inst_mask
    
    # 2. æ— çœŸå®æ©ç ï¼Œç”Ÿæˆä¼ªæ©ç ï¼ˆå¼ºåˆ¶å¯ç”¨ï¼‰
    if Config.USE_FAKE_MASK:
        inst_mask = np.zeros((img_h, img_w), dtype=np.uint8)
        x1, y1, x2, y2 = bbox
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(img_w-1, x2), min(img_h-1, y2)
        inst_mask[y1:y2, x1:x2] = 1
        if np.sum(inst_mask) >= Config.MIN_INST_AREA:
            return inst_mask
    
    return None

def get_voc_sample():
    """è·å–é‡‡æ ·å›¾ç‰‡IDåˆ—è¡¨"""
    xml_dir = os.path.join(Config.VOC2012_ROOT, "Annotations")
    xml_files = [f for f in os.listdir(xml_dir) if f.endswith(".xml")]
    if len(xml_files) < Config.SAMPLE_NUM:
        log(f"âš ï¸  XMLæ–‡ä»¶æ•°é‡ä¸è¶³ï¼Œä»…é‡‡æ ·{len(xml_files)}å¼ ")
        sample_files = xml_files
    else:
        sample_files = random.sample(xml_files, Config.SAMPLE_NUM)
    sample_ids = [f.replace(".xml", "") for f in sample_files]
    log(f"âœ… å®Œæˆé‡‡æ ·ï¼š{len(sample_ids)}å¼ å›¾ç‰‡")
    return sample_ids

# ====================== 4. SAMæ ¸å¿ƒé€»è¾‘ï¼ˆç®€åŒ–+é˜²å´©æºƒï¼‰ ======================
def load_sam_model():
    """åŠ è½½SAMæ¨¡å‹"""
    try:
        sam = sam_model_registry[Config.MODEL_TYPE](checkpoint=Config.SAM_CKPT)
        sam.to(Config.DEVICE)
        sam.eval()
        predictor = SamPredictor(sam)
        log("âœ… SAMæ¨¡å‹åŠ è½½å®Œæˆ")
        return predictor
    except Exception as e:
        log(f"âŒ åŠ è½½SAMå¤±è´¥ï¼š{str(e)}")
        exit(1)

def get_mask_centroid(mask):
    """è·å–æ©ç è´¨å¿ƒ"""
    y, x = np.where(mask == 1)
    if len(x) == 0:
        return (mask.shape[1]//2, mask.shape[0]//2)  # å…œåº•ï¼šè¿”å›ä¸­å¿ƒ
    return (int(np.mean(x)), int(np.mean(y)))

def generate_negative_point(bbox, img_w, img_h):
    """ç”Ÿæˆè´Ÿç‚¹ï¼ˆç®€å•ç‰ˆï¼‰"""
    x1, y1, x2, y2 = bbox
    for _ in range(10):
        px = random.randint(0, img_w-1)
        py = random.randint(0, img_h-1)
        if not (x1 <= px <= x2 and y1 <= py <= y2):
            return (px, py)
    return (0, 0)  # å…œåº•

def calculate_iou(pred, gt):
    """è®¡ç®—IoU"""
    intersection = np.logical_and(pred, gt).sum()
    union = np.logical_or(pred, gt).sum()
    return intersection / union if union > 0 else 0.0

def get_instance_features(bbox, mask, img_w, img_h):
    """è®¡ç®—å®ä¾‹ç‰¹å¾ï¼ˆå°ºå¯¸+å½¢çŠ¶ï¼‰"""
    # å°ºå¯¸ç‰¹å¾ï¼šå®ä¾‹é¢ç§¯/å›¾ç‰‡é¢ç§¯
    x1, y1, x2, y2 = bbox
    inst_area = (x2-x1)*(y2-y1)
    size_feat = inst_area / (img_w * img_h)
    
    # å½¢çŠ¶ç‰¹å¾ï¼š(å‘¨é•¿Â²)/é¢ç§¯ï¼ˆè¶Šä¸è§„åˆ™å€¼è¶Šå¤§ï¼‰
    mask_uint8 = (mask * 255).astype(np.uint8)
    contours, _ = cv2.findContours(mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        shape_feat = 1000.0
    else:
        max_cnt = max(contours, key=cv2.contourArea)
        perimeter = cv2.arcLength(max_cnt, closed=True)
        area = cv2.contourArea(max_cnt)
        shape_feat = (perimeter**2)/area if area > 0 else 1000.0
    
    return size_feat, shape_feat

def predict_with_sam(predictor, img_rgb, points, labels):
    """SAMæ¨ç†ï¼ˆç»Ÿä¸€å°è£…ï¼‰"""
    try:
        predictor.set_image(img_rgb)
        with torch.no_grad():
            masks, scores, _ = predictor.predict(
                point_coords=points,
                point_labels=labels,
                multimask_output=True
            )
        # é€‰æœ€ä¼˜æ©ç 
        best_idx = np.argmax(scores)
        best_mask = masks[best_idx].cpu().numpy() if torch.is_tensor(masks) else masks[best_idx]
        # ç¼©æ”¾è‡³å›¾ç‰‡å°ºå¯¸
        best_mask = cv2.resize(best_mask.astype(np.uint8), 
                              (img_rgb.shape[1], img_rgb.shape[0]), 
                              interpolation=cv2.INTER_NEAREST)
        return (best_mask > 0.5).astype(np.uint8)
    except Exception as e:
        log(f"âš ï¸ SAMæ¨ç†å¤±è´¥ï¼š{str(e)}")
        return None

# ====================== 5. ä¸»å®éªŒæµç¨‹ï¼ˆç®€åŒ–+é€æ­¥è°ƒè¯•ï¼‰ ======================
def run_experiment():
    # 1. åˆå§‹åŒ–
    init_env()
    sample_ids = get_voc_sample()
    predictor = load_sam_model()
    
    # 2. é¢„è®¡ç®—å¤æ‚åº¦é˜ˆå€¼ï¼ˆé˜²ç©ºåˆ—è¡¨ï¼‰
    log("ğŸ“ˆ é¢„è®¡ç®—å®ä¾‹å¤æ‚åº¦é˜ˆå€¼...")
    size_feats, shape_feats = [], []
    for img_id in tqdm(sample_ids, desc="é¢„è®¡ç®—é˜ˆå€¼"):
        xml_path = os.path.join(Config.VOC2012_ROOT, "Annotations", f"{img_id}.xml")
        img_info, instances = parse_voc_xml(xml_path)
        if not img_info or not instances:
            continue
        img_w, img_h = img_info["w"], img_info["h"]
        for inst in instances:
            mask = get_instance_mask(img_info["filename"], inst["bbox"], img_h, img_w)
            if mask is None:
                continue
            size_feat, shape_feat = get_instance_features(inst["bbox"], mask, img_w, img_h)
            size_feats.append(size_feat)
            shape_feats.append(shape_feat)
    
    # é˜ˆå€¼é»˜è®¤å€¼ï¼ˆé¿å…ç©ºåˆ—è¡¨æŠ¥é”™ï¼‰
    size_thresh = np.quantile(size_feats, 0.25) if size_feats else 0.01
    shape_thresh = np.quantile(shape_feats, 0.75) if shape_feats else 1000.0
    log(f"âœ… é˜ˆå€¼è®¡ç®—å®Œæˆï¼šsize_thresh={size_thresh:.4f}, shape_thresh={shape_thresh:.2f}")
    
    # 3. é€å›¾æ¨ç†
    log("\nğŸš€ å¼€å§‹é€å›¾æ¨ç†...")
    global_point_only = []
    global_adaptive = []
    complex_img_num = 0
    simple_img_num = 0
    
    for idx, img_id in enumerate(tqdm(sample_ids, desc="æ¨ç†è¿›åº¦")):
        # åˆå§‹åŒ–å•å›¾ç»“æœ
        img_res = {
            "img_id": img_id,
            "valid_inst": 0,
            "hard_inst": 0,
            "hard_ratio": 0.0,
            "is_complex": False,
            "point_only_miou": 0.0,
            "adaptive_miou": 0.0,
            "improvement": 0.0
        }
        
        # åŠ è½½å›¾ç‰‡å’Œæ ‡æ³¨
        xml_path = os.path.join(Config.VOC2012_ROOT, "Annotations", f"{img_id}.xml")
        img_info, instances = parse_voc_xml(xml_path)
        if not img_info or not instances:
            log(f"âš ï¸  {img_id} æ— æœ‰æ•ˆæ ‡æ³¨ï¼Œè·³è¿‡")
            continue
        
        img_path = os.path.join(Config.VOC2012_ROOT, "JPEGImages", img_info["filename"])
        img = cv2.imread(img_path)
        if img is None:
            log(f"âš ï¸  {img_id} å›¾ç‰‡è¯»å–å¤±è´¥ï¼Œè·³è¿‡")
            continue
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_w, img_h = img_info["w"], img_info["h"]
        
        # é€å®ä¾‹å¤„ç†
        point_only_ious = []
        adaptive_ious = []
        hard_count = 0
        valid_count = 0
        
        for inst in instances:
            # è·å–æ©ç 
            mask = get_instance_mask(img_info["filename"], inst["bbox"], img_h, img_w)
            if mask is None:
                continue
            valid_count += 1
            
            # 1. Point-only æç¤ºï¼ˆè´¨å¿ƒ+è´Ÿç‚¹ï¼‰
            centroid = get_mask_centroid(mask)
            neg_point = generate_negative_point(inst["bbox"], img_w, img_h)
            points = np.array([centroid, neg_point], dtype=np.float32)
            labels = np.array([1, 0], dtype=np.int32)
            pred_mask = predict_with_sam(predictor, img_rgb, points, labels)
            if pred_mask is None:
                continue
            iou1 = calculate_iou(pred_mask, mask)
            point_only_ious.append(iou1)
            
            # 2. è‡ªé€‚åº”æç¤ºï¼ˆéš¾ä¾‹å¤šåŠ ç‚¹ï¼Œæ˜“ä¾‹å•ç‚¹ï¼‰
            size_feat, shape_feat = get_instance_features(inst["bbox"], mask, img_w, img_h)
            is_hard = (size_feat < size_thresh) or (shape_feat > shape_thresh) or (inst["difficult"] == 1)
            if is_hard:
                hard_count += 1
                # éš¾ä¾‹ï¼šå¤šæ ¸å¿ƒç‚¹+é«˜å“åº”ç‚¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
                adaptive_points = [centroid]
                # åŠ ä¸¤ä¸ªéšæœºæ­£ç‚¹ï¼ˆç®€åŒ–è‡ªé€‚åº”é€»è¾‘ï¼Œä¿è¯èƒ½è¿è¡Œï¼‰
                for _ in range(2):
                    y = random.randint(inst["bbox"][1], inst["bbox"][3])
                    x = random.randint(inst["bbox"][0], inst["bbox"][2])
                    adaptive_points.append((x, y))
                adaptive_points = np.array(adaptive_points[:3], dtype=np.float32)
            else:
                # æ˜“ä¾‹ï¼šä»…è´¨å¿ƒ
                adaptive_points = np.array([centroid], dtype=np.float32)
            adaptive_labels = np.array([1]*len(adaptive_points), dtype=np.int32)
            
            pred_mask2 = predict_with_sam(predictor, img_rgb, adaptive_points, adaptive_labels)
            if pred_mask2 is None:
                continue
            iou2 = calculate_iou(pred_mask2, mask)
            adaptive_ious.append(iou2)
        
        # å•å›¾ç»“æœç»Ÿè®¡
        if valid_count > 0:
            img_res["valid_inst"] = valid_count
            img_res["hard_inst"] = hard_count
            img_res["hard_ratio"] = round(hard_count / valid_count, 4)
            img_res["is_complex"] = hard_count / valid_count >= 0.5
            
            if point_only_ious:
                img_res["point_only_miou"] = round(np.mean(point_only_ious), 4)
                global_point_only.extend(point_only_ious)
            if adaptive_ious:
                img_res["adaptive_miou"] = round(np.mean(adaptive_ious), 4)
                global_adaptive.extend(adaptive_ious)
            
            img_res["improvement"] = round(img_res["adaptive_miou"] - img_res["point_only_miou"], 4)
            
            if img_res["is_complex"]:
                complex_img_num += 1
            else:
                simple_img_num += 1
        
        # å†™å…¥å•å›¾ç»“æœ
        with open(os.path.join(Config.OUTPUT_DIR, "image_details.csv"), "a", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                img_res["img_id"], img_res["valid_inst"], img_res["hard_inst"],
                img_res["hard_ratio"], img_res["is_complex"], img_res["point_only_miou"],
                img_res["adaptive_miou"], img_res["improvement"]
            ])
        
        # æ¸…ç©ºæ˜¾å­˜
        if Config.DEVICE == "cuda":
            torch.cuda.empty_cache()
    
    # 4. å…¨å±€æ±‡æ€»
    summary = {
        "æ€»é‡‡æ ·æ•°": len(sample_ids),
        "æœ‰æ•ˆå›¾ç‰‡æ•°": complex_img_num + simple_img_num,
        "å¤æ‚å›¾ç‰‡æ•°": complex_img_num,
        "ç®€å•å›¾ç‰‡æ•°": simple_img_num,
        "Point-onlyå…¨å±€mIoU": round(np.mean(global_point_only), 4) if global_point_only else 0.0,
        "è‡ªé€‚åº”Pointå…¨å±€mIoU": round(np.mean(global_adaptive), 4) if global_adaptive else 0.0,
        "å¹³å‡æ€§èƒ½æå‡": round(np.mean(global_adaptive) - np.mean(global_point_only), 4) if (global_point_only and global_adaptive) else 0.0
    }
    
    # æ‰“å°æ±‡æ€»ç»“æœ
    log("\n" + "="*50)
    log("ğŸ“Š å®éªŒæ±‡æ€»ç»“æœ")
    for k, v in summary.items():
        log(f"{k}: {v}")
    save_summary(summary)
    log("\nâœ… å®éªŒå®Œæˆï¼ç»“æœæ–‡ä»¶è·¯å¾„ï¼š")
    log(f"  - å•å›¾è¯¦æƒ…ï¼š{os.path.join(Config.OUTPUT_DIR, 'image_details.csv')}")
    log(f"  - æ±‡æ€»ç»“æœï¼š{os.path.join(Config.OUTPUT_DIR, 'summary.csv')}")
    log(f"  - æ—¥å¿—æ–‡ä»¶ï¼š{os.path.join(Config.OUTPUT_DIR, 'log.txt')}")

# ====================== è¿è¡Œå…¥å£ ======================
if __name__ == "__main__":
    # ä¾èµ–æ£€æŸ¥
    try:
        import segment_anything
    except ImportError:
        print("âŒ ç¼ºå°‘segment-anythingåº“ï¼Œè¯·æ‰§è¡Œï¼špip install segment-anything")
        exit(1)
    
    # è·¯å¾„æ£€æŸ¥
    if not os.path.exists(Config.VOC2012_ROOT):
        print(f"âŒ VOCè·¯å¾„ä¸å­˜åœ¨ï¼š{Config.VOC2012_ROOT}")
        exit(1)
    if not os.path.exists(Config.SAM_CKPT):
        print(f"âŒ SAMæƒé‡ä¸å­˜åœ¨ï¼š{Config.SAM_CKPT}")
        exit(1)
    
    # è¿è¡Œå®éªŒ
    run_experiment()